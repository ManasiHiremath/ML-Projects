{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cadd64ae",
   "metadata": {},
   "source": [
    "## Project Objective: AUTOMATIC TICKET ASSIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81240a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl\n",
    "# !pip install tabulate\n",
    "# !pip install langdetect\n",
    "# !pip install flake8 pycodestyle_magic\n",
    "# !pip install textblob\n",
    "# !pip install pycountry\n",
    "# !pip install spacy_cld\n",
    "# !pip install fasttext\n",
    "# !pip install fastlangid\n",
    "# !pip install plotly\n",
    "# !pip install chart_studio\n",
    "# %reset\n",
    "# !pip install wordcloud\n",
    "#!pip install wordcloud\n",
    "# !pip install lightgbm\n",
    "# !pip install -U gensim\n",
    "# !pip install yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64424047",
   "metadata": {},
   "source": [
    "### Process Flow\n",
    "1. Import Data\n",
    "2. Explore Data, check for any inconsitancy\n",
    "3. Remove redundant data from short Description and Description.\n",
    "4. detect language of description.\n",
    "5. EDA and Visulization\n",
    "7. Drop duplicates based on shot description , description and assignment group\n",
    "8. tokenize data and remove stop words.\n",
    "9. create corpus for Description and combined Description.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471691ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04715556",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# File: Autometic Ticket Assignment\n",
    "# Author: Manasi Hiremath\n",
    "# Date: 31-Aug-21\n",
    "# Purpose: NLP based Ticket Assignment\n",
    "# Input : Input.xlsx \n",
    "# Output:Predictive model to automate ticket assignment\n",
    "# Total Execution Time:495.24 second(s)\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a48020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PEP8 Standards\n",
    "# %reload_ext pycodestyle_magic\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd6b8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ===================== Step 1: Import library =================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import openpyxl\n",
    "import re\n",
    "import time\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "import pytz\n",
    "from fastlangid.langid import LID\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import logging\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bce13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================== Step 2: Logging =================\n",
    "# logging.basicConfig(filename='NLP_Capstone_Project.log', level=logging.DEBUG,\n",
    "#                     format='%(process)d| %(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
    "# logging.warning('This will get logged to a file')\n",
    "# logging.info('This will get logged to a file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a18bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Step 3: Processing =================\n",
    "def func_read_data_n_explore(path):\n",
    "    '''\n",
    "    This Function takes input excel file, converts it into pandas dataframe and carry out intial data exploration\n",
    "    input parameter: input file path\n",
    "    output parameter: Summary of Data exploration and output pandas dataframe\n",
    "    '''\n",
    "    temp_01 = pd.read_excel(path, engine='openpyxl')\n",
    "    data_with_missing_obs = [x for x in temp_01.columns if len(temp_01[temp_01[x].isnull()]) >= 1]\n",
    "    print('Data Summary')\n",
    "    print('')\n",
    "    print('1.Shape of Data:{}'.format(temp_01.shape))\n",
    "    print('')\n",
    "    print('2.Display if column has null values:{}'.format(data_with_missing_obs))\n",
    "    print('')\n",
    "    print('3.Display count of missing data :{}'.format(temp_01.isnull().sum()))\n",
    "    print('')\n",
    "    for i in range(len(data_with_missing_obs)):\n",
    "        print(\"4.{} Sample of missing column:{}\".format(i, data_with_missing_obs[i]))\n",
    "        print('****************')\n",
    "        temp_02 = temp_01[temp_01[data_with_missing_obs[i]].isnull()]\n",
    "        print(temp_02.head())\n",
    "        print('')\n",
    "    return temp_01\n",
    "\n",
    "\n",
    "def func_remov_special_char(text):\n",
    "    '''\n",
    "    Remove Special Characters from text\n",
    "    input: text. i.e dataframe column values\n",
    "    output: clean text\n",
    "    '''\n",
    "    try:\n",
    "        return re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "def func_remov_disclaimer(text):\n",
    "    '''\n",
    "    Remove Disclaimer from text\n",
    "    input: text. i.e dataframe column values\n",
    "    output: clean text\n",
    "    '''\n",
    "    try:\n",
    "        return re.sub(r\"select the following link to view the disclaimer in an alternate language. \",\"\", text)\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def func_clean_data(temp_02):\n",
    "    # Remove new line characters\n",
    "    temp_02['Description'] = temp_02['Description'].apply(func_remov_disclaimer)\n",
    "    temp_02['Short description'] = temp_02['Description'].apply(func_remov_disclaimer)    \n",
    "    remove_char_list = ['_x000D_\\n', '_x000D_\\n_x000D_\\n','received from:', r\"\\S*@\\S*\\s\", '\\n']\n",
    "    temp_02['Description'] = temp_02['Description'].replace(remove_char_list, '', regex=True)\n",
    "    temp_02['Short description'] = temp_02['Short description'].replace(remove_char_list, '', regex=True)\n",
    "    temp_02['Short description'] = temp_02['Short description'].apply(func_remov_special_char)\n",
    "    temp_02['Description'] = temp_02['Description'].apply(func_remov_special_char)\n",
    "    # Remove caller name from description\n",
    "    temp_03 = pd.DataFrame(temp_02.Caller.str.split(' ', 2).tolist(),columns=['FirstName', 'LastName'])\n",
    "    temp_03.update(temp_03[['FirstName', 'LastName']].applymap('{}'.format))\n",
    "    for name in range(len(temp_03)):\n",
    "        temp_02['Description'] = temp_02['Description'].str.replace(temp_03['FirstName'][name], '')\n",
    "        temp_02['Description'] = temp_02['Description'].str.replace(temp_03['LastName'][name], '')\n",
    "        temp_02['Short description'] = temp_02['Short description'].str.replace(temp_03['FirstName'][name], '')\n",
    "        temp_02['Short description'] = temp_02['Short description'].str.replace(temp_03['LastName'][name], '')\n",
    "    # strip white space\n",
    "    temp_02[temp_02.columns] = temp_02.apply(lambda x: x.str.strip())\n",
    "    #remove extra space between words\n",
    "    temp_02['Description'] = [(\" \".join(temp_02['Description'][x].split())) for x in range(len(temp_02))]\n",
    "    temp_02['Short description'] = [(\" \".join(temp_02['Short description'][x].split())) for x in range(len(temp_02))]\n",
    "    return temp_02\n",
    "\n",
    "\n",
    "# Funtion to detect text language\n",
    "def func_detect_text_language(text):\n",
    "    '''\n",
    "    This function is used to detect the language of the text\n",
    "    input parameter: text\n",
    "    Output value\": language of the text\n",
    "    '''\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception:\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "def func_vis01_target_analysis(temp05):\n",
    "    '''\n",
    "    This Function creates visualization.\n",
    "    input: processed input data\n",
    "    output visulizations\n",
    "    '''\n",
    "    # Assignment Column analysis\n",
    "    temp06 = temp05.groupby(['Assignment group'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "    temp06 = temp06.sort_values(by='Total_Count', axis=0, ascending=False)\n",
    "    temp06['Ticket_Contribution'] = round((temp06['Total_Count']/temp06['Total_Count'].sum())*100, 2)\n",
    "    # Check Contirbution of each assignemt group\n",
    "    fig = px.bar(temp06, x='Assignment group', y='Ticket_Contribution', title=\"Ticket Contribution by Assignment Group\",\n",
    "                 hover_data=['Assignment group', 'Total_Count', 'Ticket_Contribution'])\n",
    "    fig.show()\n",
    "    \n",
    "    prod_list_with_small_data=temp06[temp06['Total_Count'] <2 ][['Assignment group', 'Total_Count']]\n",
    "    print(prod_list_with_small_data)\n",
    "    \n",
    "    temp06 = temp05.groupby(['Assignment group'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "    temp06 = temp06.sort_values(by='Total_Count', axis=0, ascending=False)\n",
    "    temp06['Ticket_Contribution'] = round((temp06['Total_Count']/temp06['Total_Count'].sum())*100, 2)\n",
    "    # Check Contirbution of each assignemt group\n",
    "    fig = px.bar(temp06, x='Assignment group', y='Ticket_Contribution', title=\"Ticket Contribution by Assignment Group\",\n",
    "                 hover_data=['Assignment group', 'Total_Count', 'Ticket_Contribution'])\n",
    "    fig.show()\n",
    "    \n",
    "    # Language analysis\n",
    "    temp07 = temp05.groupby(['Language_name'])['Language_name'].count().to_frame('Total_Count').reset_index()\n",
    "    temp07 = temp07.sort_values(by='Total_Count', axis=0, ascending=False)\n",
    "    temp07['Language_used'] = round((temp07['Total_Count']/temp07['Total_Count'].sum())*100, 2)\n",
    "    # Check Contirbution of each assignemt group\n",
    "    fig = px.bar(temp07, x='Language_name', y='Language_used', title=\"Distribution of Languages used by caller\",\n",
    "                 hover_data=['Language_name', 'Total_Count', 'Language_used'])\n",
    "    fig.show()\n",
    "    # Languages contribution within assingment Group\n",
    "    temp08 = temp05.groupby(['Assignment group','Language_name'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "    temp08 = temp08.sort_values(by='Total_Count', axis=0, ascending=False)\n",
    "    temp08['Ticket_Contribution'] = round((temp08['Total_Count']/temp08['Total_Count'].sum())*100, 2)\n",
    "    fig = px.bar(temp08, x='Assignment group', y='Ticket_Contribution', title=\"Languages used in Assignment Group\",\n",
    "                 hover_data=['Assignment group', 'Total_Count', 'Ticket_Contribution'], color='Language_name')\n",
    "    fig.show()\n",
    "    # Checking length of ticket short descriptions\n",
    "    result = [len(x) for x in temp05['Short description']]\n",
    "    print('Sample Length of ticket Short Description:',result[0:10])\n",
    "    print('Maximum ticket Description length:',np.max(result))\n",
    "    print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "    fig = px.box(y=result,title=\"Length Distribution of Short description of ticket\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Checking length of ticket descriptions\n",
    "    result = [len(x) for x in temp05['Description']]\n",
    "    print('Sample Length of ticket Short Description:',result[0:10])\n",
    "    print('Maximum ticket Description length:',np.max(result))\n",
    "    print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "    fig = px.box(y=result,title=\"Length Distribution of ticket Description\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Checking length of combined description\n",
    "    temp_05.loc[:,'combined_description']=temp_05['Short description'].astype(str)+' '+temp_05['Description'].astype(str)\n",
    "    result = [len(x) for x in temp05['combined_description']]\n",
    "    print('Sample Length of ticket Short Description:',result[0:10])\n",
    "    print('Maximum ticket Description length:',np.max(result))\n",
    "    print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "    fig = px.box(y=result,title=\"Length Distribution of Short and long description\")\n",
    "    fig.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "stop_words=nltk.corpus.stopwords.words('english')+ list(string.punctuation)\n",
    "def func_doc_preprocess(doc):\n",
    "    '''\n",
    "    This function tokenize text data.\n",
    "    Removes stop words from text.\n",
    "    input parameter: doc: text \n",
    "    output parameter: corpus of token\n",
    "    '''\n",
    "    doc = re.sub(r'[^a-zA-Z\\s^\\D]', ' ', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    ## tokenize the document\n",
    "    tokens= nltk.word_tokenize(doc)\n",
    "    #remove stop words\n",
    "    wordList = [token for token in tokens if token not in stop_words and not token.isdigit()]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(wordList)\n",
    "    return doc\n",
    "\n",
    "def func_feature_importance_by_class(data,corpus):\n",
    "    '''\n",
    "    This function proviesd important features for each assignement group.\n",
    "    input parameter: data: clean input data.\n",
    "                     Corpus:tokenize data\n",
    "    ouput: provides important features for each traget class.\n",
    "    '''\n",
    "    dummy_vars=pd.get_dummies(data['Assignment group'])\n",
    "    data_10=pd.concat([data,dummy_vars], axis=1)\n",
    "    \n",
    "    # tf_idf\n",
    "    vectorizer = TfidfVectorizer(stop_words ='english', \n",
    "                              ngram_range = (1,3), \n",
    "                              max_df = .6, min_df = .01,\n",
    "                              sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X=X.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    tfidf = pd.DataFrame(np.round(X, 3), columns=feature_names)\n",
    "    \n",
    "    model = ExtraTreesClassifier()\n",
    "    feat_importances_df=pd.DataFrame()\n",
    "    # create a var name\n",
    "    for i in range(len(data['Assignment group'].unique())):\n",
    "        var='GRP'+'_'+str(i)\n",
    "        labels=data_10[var]\n",
    "        model.fit(tfidf, labels)\n",
    "        feat_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                    index=vectorizer.get_feature_names(),\n",
    "                                    columns=['feature_importances'])\n",
    "        feat_importances.reset_index(inplace=True)\n",
    "        feat_importances.rename(columns={\"index\": \"features\"})\n",
    "        feat_importances['Target_class']=var\n",
    "        feat_importances1=feat_importances.sort_values(by=['Target_class','feature_importances'], ascending=False).head(10)\n",
    "        print(\"\\n==> \" ,var)\n",
    "        print(\"  * Most Correlated terms are: %s\" %(', '.join(feat_importances1['index'])))\n",
    "\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "def train_test_fit(model, X_train, X_test, y_train, y_test): \n",
    "    '''\n",
    "    This function fits the given model and score its performance\n",
    "    input parameter:model : specify classification algorithm for experiment\n",
    "                    X_train : Feature train data\n",
    "                    X_test : Feature test data\n",
    "                    y_train : Labels train data\n",
    "                    y_test : Labels test data\n",
    "    Output parameter:y_pred: predicted lables\n",
    "                     score: model score for test data\n",
    "                     recall\n",
    "                     precision\n",
    "                     f1_score\n",
    "                     duration    \n",
    "    '''\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)   # fit the model with the train data\n",
    "    y_pred = model.predict(X_test)  # make predictions on the test set\n",
    "    score = round(model.score(X_test, y_test), 3)   # compute accuracy score for test set\n",
    "    \n",
    "    recall=metrics.recall_score(y_test, y_pred,average='weighted',labels=np.unique(y_pred))\n",
    "    precision=metrics.precision_score(y_test, y_pred,average='weighted',labels=np.unique(y_pred))\n",
    "    f1_score= round(metrics.f1_score(y_test, y_pred,average='weighted',labels=np.unique(y_pred)),3)\n",
    "    end = time.time()\n",
    "    duration = end - start  # calculate the total duration\n",
    "    \n",
    "    return y_pred,score, recall, precision,f1_score,duration   # return all the metrics\n",
    "\n",
    "\n",
    "def model_training(classfierdict, X_train, X_test, y_train, y_test,iteration_det):\n",
    "    '''\n",
    "    This function creates a table with scoring metric\n",
    "    input parameter:classfierdict : specify classification algorithm for experiment\n",
    "                    X_train : Feature train data\n",
    "                    X_test : Feature test data\n",
    "                    y_train : Labels train data\n",
    "                    y_test : Labels test data\n",
    "    Output parameter:scoring_metric: scoring metric table\n",
    "    '''\n",
    "    scoring_metric=pd.DataFrame()\n",
    "    scoring_metric_fl=pd.DataFrame()\n",
    "    model_name_list= []\n",
    "    score_list = []\n",
    "    recall_list=[]\n",
    "    precision_list=[]\n",
    "    f1_score_list=[]\n",
    "    elapsed=[]\n",
    "    y_pred_list=[]\n",
    "    cm=[]\n",
    "    \n",
    "    for i in classfierdict['model']:\n",
    "        y_pred,score, recall, precision,f1_score,duration=train_test_fit(i,X_train, X_test, y_train, y_test)\n",
    "        y_pred_list.append(y_pred)\n",
    "        model_name_list.append(i)\n",
    "        score_list.append(score)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        f1_score_list.append(f1_score)\n",
    "        elapsed.append(duration)\n",
    "        Model_name='model'+'_'+re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(i))        \n",
    "        \n",
    "    scoring_metric = pd.DataFrame({'iteration': iteration_det,\n",
    "                                   'Model':model_name_list,\n",
    "                                   'Model_score':score_list,\n",
    "                                   'Model_recall':recall_list,\n",
    "                                   'Model_precision':precision_list,\n",
    "                                   'Model_f1_score':f1_score_list,\n",
    "#                                    'Model_roc_auc':roc_auc_list,\n",
    "                                   'Elapsed': elapsed})\n",
    "    scoring_metric['Model_name']= scoring_metric['Model'].astype('str').map(lambda x:re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x))   \n",
    "    return scoring_metric  # return all the metrics\n",
    "\n",
    "\n",
    "def func_tfidx_metric(corpus,model_data):\n",
    "    '''\n",
    "    This function creates a tfidx matrix\n",
    "    input parameter:corpus : corpus \n",
    "                    model_data : analysis data\n",
    "    Output parameter:scoring_metric: scoring metric table\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(stop_words ='english', \n",
    "                              ngram_range = (1,3), \n",
    "                              max_df = .6, min_df = .01,\n",
    "                              sublinear_tf=True)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    X=X.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    tfidf = pd.DataFrame(np.round(X, 3), columns=feature_names)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                                            tfidf, model_data['Assignment group'],\n",
    "                                            test_size=0.1, random_state=10, \n",
    "                                             stratify=model_data['Assignment group'])\n",
    "    return X_tr, X_te, y_tr, y_te\n",
    "\n",
    "\n",
    "def func_feature_selection_pca(X_tr, X_te):\n",
    "    '''\n",
    "    This function selects features based on pca\n",
    "    input parameter: X_tr : train features \n",
    "                     X_te : test features \n",
    "    Output parameter:X_tr_pca:selected train features\n",
    "                     X_te_pca:selected test features\n",
    "    '''    \n",
    "    pca = PCA(0.95)\n",
    "    pca.fit(X_tr)\n",
    "    X_tr_pca = pca.transform(X_tr)\n",
    "    X_te_pca = pca.transform(X_te)\n",
    "    return X_tr_pca, X_te_pca\n",
    "\n",
    "\n",
    "def func_feature_selection_chi(X_tr, y_tr, X_te):\n",
    "    '''\n",
    "    This function selects features using chisquare\n",
    "    input parameter: X_tr : train features \n",
    "                     X_te : test features\n",
    "                     y_tr : train label\n",
    "    Output parameter:X_tr_chi2:selected train features\n",
    "                     X_te_chi2:selected test features\n",
    "    ''' \n",
    "    ch2 = SelectKBest(chi2, k=100)\n",
    "    X_tr_chi2 = ch2.fit_transform(X_tr,y_tr)\n",
    "    X_te_chi2 = ch2.transform(X_te)\n",
    "    return X_tr_chi2, X_te_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729ff679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1. Getting Input Data Information...\n",
      "Data Summary\n",
      "\n",
      "1.Shape of Data:(8500, 4)\n",
      "\n",
      "2.Display if column has null values:['Short description', 'Description']\n",
      "\n",
      "3.Display count of missing data :Short description    8\n",
      "Description          1\n",
      "Caller               0\n",
      "Assignment group     0\n",
      "dtype: int64\n",
      "\n",
      "4.0 Sample of missing column:Short description\n",
      "****************\n",
      "     Short description                                        Description  \\\n",
      "2604               NaN  _x000D_\\n_x000D_\\nreceived from: ohdrnswl.rezu...   \n",
      "3383               NaN  _x000D_\\n-connected to the user system using t...   \n",
      "3906               NaN  -user unable  tologin to vpn._x000D_\\n-connect...   \n",
      "3910               NaN  -user unable  tologin to vpn._x000D_\\n-connect...   \n",
      "3915               NaN  -user unable  tologin to vpn._x000D_\\n-connect...   \n",
      "\n",
      "                 Caller Assignment group  \n",
      "2604  ohdrnswl rezuibdt           GRP_34  \n",
      "3383  qftpazns fxpnytmk            GRP_0  \n",
      "3906  awpcmsey ctdiuqwe            GRP_0  \n",
      "3910  rhwsmefo tvphyura            GRP_0  \n",
      "3915  hxripljo efzounig            GRP_0  \n",
      "\n",
      "4.1 Sample of missing column:Description\n",
      "****************\n",
      "             Short description Description             Caller Assignment group\n",
      "4395  i am locked out of skype         NaN  viyglzfo ajtfzpkb            GRP_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "print('Step 1. Getting Input Data Information...')\n",
    "temp_03 = func_read_data_n_explore(\"input_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9605f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2. Data Cleaning has started...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3ce481aca915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step 2. Data Cleaning has started...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtemp_04\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_clean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step 3. Detecting text language...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtemp_04\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_lang'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_04\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_detect_text_language\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Get language name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6870952e12de>\u001b[0m in \u001b[0;36mfunc_clean_data\u001b[1;34m(temp_02)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mtemp_02\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_02\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_03\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LastName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mtemp_02\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Short description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_02\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Short description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_03\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FirstName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mtemp_02\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Short description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_02\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Short description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_03\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LastName'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;31m# strip white space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mtemp_02\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_02\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_02\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlpmyenv\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlpmyenv\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlpmyenv\\lib\\site-packages\\pandas\\core\\strings\\accessor.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"categories\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# categorical / normal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0minferred_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Step 2. Data Cleaning has started...\")\n",
    "temp_04 = func_clean_data(temp_03)\n",
    "print(\"Step 3. Detecting text language...\")\n",
    "temp_04['text_lang'] = temp_04['Description'].apply(func_detect_text_language)\n",
    "# Get language name\n",
    "lang = pd.read_csv('language-codes_csv.csv')\n",
    "temp_05 = pd.merge(temp_04, lang, left_on='text_lang', right_on='alpha2', how='left') \n",
    "print(\"Step 4. Data Explorarion...\")\n",
    "print('')\n",
    "print('Data has - {} - ticket assignment groups'.format(len(temp_03['Assignment group'].unique())))\n",
    "print('')\n",
    "print('Tickets are registered in - {} languages '.format(len(temp_04['text_lang'].unique()))) \n",
    "print('')\n",
    "func_vis01_target_analysis(temp_05)\n",
    "print('Step 4.1. NLP preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_05.loc[:,'combined_description']=temp_05['Short description'].astype(str)+' '+temp_05['Description'].astype(str)\n",
    "print(temp_05.shape)\n",
    "temp_06=temp_05.drop_duplicates(['Short description','Description','Assignment group'])\n",
    "print(temp_06.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9abe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_06['Description']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus_model)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import gc\n",
    "gc.collect()\n",
    "# tfIdfMatrix = tfIdfMat.todense()\n",
    "labels = temp_06['Assignment group'].tolist()\n",
    "tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(X)\n",
    "plt.figure(figsize=(16,10))\n",
    "# palette = sns.hls_palette(21, l=.6, s=.9)\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "    hue=labels,\n",
    "#     palette= palette,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(\n",
    "#     x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "#     hue=labels,\n",
    "# #     palette= palette,\n",
    "#     legend=\"full\",\n",
    "#     alpha=0.3\n",
    "# )\n",
    "\n",
    "fig = px.scatter(x=tsne_results[:,0], y=tsne_results[:,1], color=labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db550eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa612666",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_11= temp_06[temp_06['Assignment group'] != 'GRP_0']\n",
    "# temp_11=temp_06.copy()\n",
    "temp_11.head(2)\n",
    "\n",
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_11['Description']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_sub = vectorizer.fit_transform(corpus_model)\n",
    "\n",
    "clusters = KMeans(4, n_init = 30, algorithm='auto')\n",
    "clusters.fit(X_sub)\n",
    "x_pred_2=clusters.predict(X_sub)\n",
    "\n",
    "\n",
    "# # print(score)\n",
    "data=temp_11.copy()\n",
    "data['clust_group']=x_pred_2\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import gc\n",
    "gc.collect()\n",
    "# tfIdfMatrix = tfIdfMat.todense()\n",
    "labels = data['clust_group'].tolist()\n",
    "tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(X_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=tsne_results[:,0], y=tsne_results[:,1], color=labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f59394",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clust_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba61309",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_12= temp_06[temp_06['Assignment group'] == 'GRP_0']\n",
    "temp_12['clust_group']=4\n",
    "# temp_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_13= pd.concat([temp_12,data], axis=0)\n",
    "temp_13['clust_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c73d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_13['Description']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_sub = vectorizer.fit_transform(corpus_model)\n",
    "\n",
    "# clusters = KMeans(4, n_init = 30, algorithm='auto')\n",
    "# clusters.fit(X_sub)\n",
    "# x_pred_2=clusters.predict(X_sub)\n",
    "\n",
    "\n",
    "# # # print(score)\n",
    "# data1=temp_13.copy()\n",
    "# data1['clust_group']=x_pred_2\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import gc\n",
    "gc.collect()\n",
    "# tfIdfMatrix = tfIdfMat.todense()\n",
    "labels = temp_13['clust_group'].astype('category').to_list()\n",
    "tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(X_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=tsne_results[:,0], y=tsne_results[:,1], color=labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26799e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_13['clust_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list=[0,1,2,3,4]\n",
    "temp_14 = temp_13[temp_13['clust_group'].isin(cluster_list)]\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_14['Description']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_sub = vectorizer.fit_transform(corpus_model)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words ='english', \n",
    "                              ngram_range = (1,3), \n",
    "                              max_df = .6, min_df = .01,\n",
    "                              sublinear_tf=True)\n",
    "X = vectorizer.fit_transform(corpus_model)\n",
    "X=X.toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "tfidf = pd.DataFrame(np.round(X, 3), columns=feature_names)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                                            tfidf, temp_14['clust_group'],\n",
    "                                            test_size=0.1, random_state=10, \n",
    "                                             stratify=temp_14['clust_group'])\n",
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pca, X_te_pca = func_feature_selection_pca(X_tr_tfidf, X_te_tfidf)\n",
    "classfierdict={'model': [\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr_pca, X_te_pca, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_chi2, X_te_chi2 = func_feature_selection_chi(X_tr_tfidf, y_tr_tfidf, X_te_tfidf)\n",
    "classfierdict={'model': [\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr_chi2, X_te_chi2, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = SpectralClustering(n_clusters=4,assign_labels='discretize',random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_11= temp_06[temp_06['Assignment group'] == 'GRP_0']\n",
    "# temp_11=temp_06.copy()\n",
    "temp_11.head(2)\n",
    "\n",
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_11['Description']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_sub = vectorizer.fit_transform(corpus_model)\n",
    "\n",
    "clustering = SpectralClustering(n_clusters=5,assign_labels='discretize',random_state=0).fit(X_sub)\n",
    "x_pred_2=clustering.labels_\n",
    "\n",
    "# # print(score)\n",
    "data=temp_11.copy()\n",
    "data['clust_group']=x_pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faddd17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clust_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9befdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import gc\n",
    "gc.collect()\n",
    "# tfIdfMatrix = tfIdfMat.todense()\n",
    "labels = data['clust_group'].astype('category').to_list()\n",
    "tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(X_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc822ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=tsne_results[:,0], y=tsne_results[:,1], color=labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da887f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words ='english', \n",
    "                              ngram_range = (1,3), \n",
    "                              max_df = .6, min_df = .01,\n",
    "                              sublinear_tf=True)\n",
    "X = vectorizer.fit_transform(corpus_model)\n",
    "X=X.toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "tfidf = pd.DataFrame(np.round(X, 3), columns=feature_names)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                                            tfidf, data['clust_group'],\n",
    "                                            test_size=0.1, random_state=10, \n",
    "                                             stratify=data['clust_group'])\n",
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr, X_te, y_tr, y_te,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6322c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb6c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3a23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963d659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d7e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Total count by assignment group\n",
    "temp_11= temp_06[temp_06['Assignment group'] != 'GRP_0']\n",
    "temp_11.head(2)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "# true_k = 10\n",
    "# model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "# model.fit(X)\n",
    "\n",
    "# print(\"Top terms per cluster:\")\n",
    "# order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "# terms = vectorizer.get_feature_names()\n",
    "# for i in range(true_k):\n",
    "#     print(\"Cluster %d:\" % i),\n",
    "#     for ind in order_centroids[i, :10]:\n",
    "#         print(' %s' % terms[ind]),\n",
    "#     print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "cluster_range = range( 2, 10 )\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(15,8))\n",
    "for i in cluster_range:\n",
    "    '''\n",
    "    Create KMeans instance for different number of clusters\n",
    "    '''\n",
    "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "    q, mod = divmod(i, 2)\n",
    "    '''\n",
    "    Create SilhouetteVisualizer instance with KMeans instance\n",
    "    Fit the visualizer\n",
    "    '''\n",
    "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "    visualizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Step 4.1. NLP preprocessing')\n",
    "# get stopwords in english\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "# create corpus\n",
    "corpus_01 = []\n",
    "corpus_01 = temp_06['Description']\n",
    "corpus_01 = np.array(corpus_01)\n",
    "print(\"Sample Original Corpus:\", corpus_01[10])    \n",
    "norm_corpus_01 = normalize_corpus(corpus_01)\n",
    "print(\"Clean Corpus:\", norm_corpus_01[10])\n",
    "    \n",
    "# Second corpus\n",
    "corpus_02 = []\n",
    "corpus_02 = temp_06['combined_description']\n",
    "corpus_02 = np.array(corpus_02)\n",
    "print(\"Sample Original Corpus:\", corpus_02[10])    \n",
    "norm_corpus_02 = normalize_corpus(corpus_02)\n",
    "print(\"Clean Corpus:\", norm_corpus_02[10])\n",
    "\n",
    "print('Step 4.2. Feature representative of each assignment group')\n",
    "temp_08=func_feature_importance_by_class(temp_06,norm_corpus_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Total count by assignment group\n",
    "temp_11= temp_06.groupby(['Assignment group'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "#consider only assignment group where total avaiable data points are more that 15\n",
    "temp_11_list=temp_11[temp_11['Total_Count'] >2]['Assignment group']\n",
    "temp_11=temp_06[temp_06['Assignment group'].isin(temp_11_list)]\n",
    "\n",
    "print(temp_11.shape)\n",
    "    \n",
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_11['Description']\n",
    "    \n",
    "\n",
    "#===================== Experiment  groups with minimum sample Size 2 =================\n",
    "X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf=func_tfidx_metric(corpus_model,temp_11)\n",
    "X_tr_pca, X_te_pca = func_feature_selection_pca(X_tr_tfidf, X_te_tfidf)\n",
    "X_tr_chi2, X_te_chi2 = func_feature_selection_chi(X_tr_tfidf, y_tr_tfidf, X_te_tfidf)\n",
    "\n",
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab205449",
   "metadata": {},
   "outputs": [],
   "source": [
    "classfierdict={'model': [\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "data_1=model_training(classfierdict,X_tr_pca, X_te_pca, y_tr_tfidf, y_te_tfidf,'pca')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "data_1=model_training(classfierdict,X_tr_chi2, X_te_chi2, y_tr_tfidf, y_te_tfidf,'chi2')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef442edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================== Experiment  groups with minimum sample Size 20=================\n",
    "# Get Total count by assignment group\n",
    "temp_11= temp_06.groupby(['Assignment group'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "#consider only assignment group where total avaiable data points are more that 15\n",
    "temp_11_list=temp_11[temp_11['Total_Count'] >20]['Assignment group']\n",
    "temp_11=temp_06[temp_06['Assignment group'].isin(temp_11_list)]\n",
    "\n",
    "print(temp_11.shape)\n",
    "    \n",
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_11['Description']\n",
    "    \n",
    "\n",
    "#===================== Experiment  groups with minimum sample Size 2 =================\n",
    "X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf=func_tfidx_metric(corpus_model,temp_11)\n",
    "X_tr_pca, X_te_pca = func_feature_selection_pca(X_tr_tfidf, X_te_tfidf)\n",
    "X_tr_chi2, X_te_chi2 = func_feature_selection_chi(X_tr_tfidf, y_tr_tfidf, X_te_tfidf)\n",
    "\n",
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef344e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "classfierdict={'model': [\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "data_1=model_training(classfierdict,X_tr_pca, X_te_pca, y_tr_tfidf, y_te_tfidf,'pca')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "data_1=model_training(classfierdict,X_tr_chi2, X_te_chi2, y_tr_tfidf, y_te_tfidf,'chi2')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Total count by assignment group\n",
    "temp_11= temp_06.groupby(['Assignment group'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "#consider only assignment group where total avaiable data points are more that 15\n",
    "temp_11_list=temp_11[temp_11['Total_Count'] >2]['Assignment group']\n",
    "temp_11=temp_06[temp_06['Assignment group'].isin(temp_11_list)]\n",
    "\n",
    "print(temp_11.shape)\n",
    "    \n",
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_11['combined_description']\n",
    "    \n",
    "\n",
    "#===================== Experiment  groups with minimum sample Size 2 =================\n",
    "X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf=func_tfidx_metric(corpus_model,temp_11)\n",
    "X_tr_pca, X_te_pca = func_feature_selection_pca(X_tr_tfidf, X_te_tfidf)\n",
    "X_tr_chi2, X_te_chi2 = func_feature_selection_chi(X_tr_tfidf, y_tr_tfidf, X_te_tfidf)\n",
    "\n",
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Total count by assignment group\n",
    "temp_11= temp_06.groupby(['Assignment group'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "#consider only assignment group where total avaiable data points are more that 15\n",
    "temp_11_list=temp_11[temp_11['Total_Count'] >50]['Assignment group']\n",
    "temp_11=temp_06[temp_06['Assignment group'].isin(temp_11_list)]\n",
    "\n",
    "print(temp_11.shape)\n",
    "    \n",
    "#create corpus for modelling\n",
    "normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "corpus_model = temp_11['combined_description']\n",
    "    \n",
    "\n",
    "#===================== Experiment  groups with minimum sample Size 2 =================\n",
    "X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf=func_tfidx_metric(corpus_model,temp_11)\n",
    "X_tr_pca, X_te_pca = func_feature_selection_pca(X_tr_tfidf, X_te_tfidf)\n",
    "X_tr_chi2, X_te_chi2 = func_feature_selection_chi(X_tr_tfidf, y_tr_tfidf, X_te_tfidf)\n",
    "\n",
    "classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "print('Step 6. Model Execution...')\n",
    "data_1=model_training(classfierdict,X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91157c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "lda_model = gensim.models.LdaMulticore(corpus_model, num_topics=10,  passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start = time.perf_counter()\n",
    "    print('Step 1. Getting Input Data Information...')\n",
    "    temp_03 = func_read_data_n_explore(\"input_data.xlsx\")\n",
    "    print(\"Step 2. Data Cleaning has started...\")\n",
    "    temp_04 = func_clean_data(temp_03)\n",
    "    print(\"Step 3. Detecting text language...\")\n",
    "    temp_04['text_lang'] = temp_04['Description'].apply(func_detect_text_language)\n",
    "    # Get language name\n",
    "    lang = pd.read_csv('language-codes_csv.csv')\n",
    "    temp_05 = pd.merge(temp_04, lang, left_on='text_lang', right_on='alpha2', how='left') \n",
    "    temp_05=temp_05.drop_duplicates(['Short description','Description','Assignment group'])\n",
    "    # Combine description and short description columns\n",
    "    temp_05.loc[:,'combined_description']=temp_05['Short description'].astype(str)+' '+temp_05['Description'].astype(str)\n",
    "    print(temp_05.shape)\n",
    "    print(\"Step 4. Data Explorarion...\")\n",
    "    print('')\n",
    "    print('Data has - {} - ticket assignment groups'.format(len(temp_03['Assignment group'].unique())))\n",
    "    print('')\n",
    "    print('Tickets are registered in - {} languages '.format(len(temp_04['text_lang'].unique()))) \n",
    "    print('')\n",
    "    func_vis01_target_analysis(temp_04)\n",
    "    print('Step 4.1. NLP preprocessing')\n",
    "    # get stopwords in english\n",
    "    normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "    # create corpus\n",
    "    corpus_01 = []\n",
    "    corpus_01 = temp_05['Description']\n",
    "    corpus_01 = np.array(corpus_01)\n",
    "    print(\"Sample Original Corpus:\", corpus_01[10])    \n",
    "    norm_corpus_01 = normalize_corpus(corpus_01)\n",
    "    print(\"Clean Corpus:\", norm_corpus_01[10])\n",
    "    \n",
    "    # Second corpus\n",
    "    corpus_02 = []\n",
    "    corpus_02 = temp_05['combined_description']\n",
    "    corpus_02 = np.array(corpus_02)\n",
    "    print(\"Sample Original Corpus:\", corpus_02[10])    \n",
    "    norm_corpus_01 = normalize_corpus(corpus_02)\n",
    "    print(\"Clean Corpus:\", norm_corpus_02[10])\n",
    "    \n",
    "    print('Step 4.2. Feature representative of each assignment group')\n",
    "    temp_08=func_feature_importance_by_class(temp_05,norm_corpus_01)\n",
    "    \n",
    "    # Get Total count by assignment group\n",
    "    temp_11= temp_05.groupby(['Assignment group'])['Assignment group'].count().to_frame('Total_Count').reset_index()\n",
    "    #consider only assignment group where total avaiable data points are more that 15\n",
    "    temp_11_list=temp_11[temp_11['Total_Count'] >15]['Assignment group']\n",
    "    temp_11=temp_05[temp_05['Assignment group'].isin(temp_11_list)]\n",
    "    \n",
    "#     #create corpus for modelling\n",
    "#     normalize_corpus = np.vectorize(func_doc_preprocess)\n",
    "#     corpus_model = temp_11['Description']\n",
    "    \n",
    "    # Feature selection\n",
    "    print('Step 5. Feature selection...')\n",
    "    X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf=func_tfidx_metric(corpus_model,temp_11)\n",
    "    \n",
    "    classfierdict={'model': [MultinomialNB(),\n",
    "                             LogisticRegression(solver='liblinear'),\n",
    "                             KNeighborsClassifier(n_neighbors=3,weights='distance'),\n",
    "                             RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             LGBMClassifier(objective='multiclass', random_state=5)]}\n",
    "    \n",
    "    print('Step 6. Model Execution...')\n",
    "    data_1=model_training(classfierdict,X_tr_tfidf, X_te_tfidf, y_tr_tfidf, y_te_tfidf,'tfidf')\n",
    "    \n",
    "    \n",
    "    X_tr_pca, X_te_pca = func_feature_selection_pca(X_tr_tfidf, X_te_tfidf)\n",
    "    X_tr_chi2, X_te_chi2 = func_feature_selection_chi(X_tr_tfidf, y_tr_tfidf, X_te_tfidf)\n",
    "    \n",
    "    finish = time.perf_counter()\n",
    "    print(f'Total Execution Time: Finished in {round(finish-start, 2)} second(s)')\n",
    "    return temp_05, norm_corpus,temp_08,data_1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
